
\documentclass[a4paper, 11pt]{article}

\usepackage[margin=2cm]{geometry}

\usepackage{amsfonts, amsmath, amssymb, amsthm}
%\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[caption=false]{subfig}


\numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}	[equation]	{Theorem}
\newtheorem{conjecture}	[equation]	{Conjecture}
\newtheorem{corollary}	[equation]	{Corollary}
\newtheorem{definition}	[equation]	{Definition}
\newtheorem{example}	[equation]	{Example}
\newtheorem{lemma}		[equation]	{Lemma}
\newtheorem{problem}	[equation]	{Problem}
\newtheorem{proposition}[equation]	{Proposition}
\newtheorem{remark}		[equation]	{Remark}

\theoremstyle{definition}
\newtheorem{claim}		[equation]	{Claim}
\newtheorem{exercise}   {Exercise}  [section]
\newtheorem{answer}     {Answer}    [section]

\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

\newcommand{\GF}{\mathrm{GF}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\dH}{d_\mathrm{H}}
\newcommand{\dmin}{d_{\min}}
\newcommand{\wH}{w_\mathrm{H}}
\newcommand{\wmin}{w_{\min}}
\newcommand{\lcm}{\mathrm{lcm}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}

\renewcommand{\i}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\divides}{\,|\,}

\newcommand{\probability}{\mathbb{P}}
\newcommand{\expectation}{\mathbb{E}}


	\newcommand{\Important}[1]{\textcolor{red}{#1}}
	\newcommand{\Structure}[1]{\textcolor{blue}{#1}}
	\newcommand{\Reference}[1]{\textcolor{teal}{#1}}
	\newcommand{\Define}[1]{\textcolor{purple}{#1}}


\newcommand{\alphabet}[1]{\mathcal{#1}}

\newcommand{\code}[1]{\mathsf{#1}}
\newcommand{\Paritycheck}           {\code{PC}}
\newcommand{\Repetition}            {\code{R}}
\newcommand{\Hamming}               {\code{H}}
\newcommand{\Golay}                 {\code{G}}
\newcommand{\ReedMuller}            {\code{RM}}
\newcommand{\ReedSolomon}           {\code{RS}}
\newcommand{\GeneralizedReedSolomon}{\code{GRS}}
\newcommand{\Alternant}             {\code{A}}
\newcommand{\Goppa}                 {\code{\Gamma}}


\renewcommand{\thesection}{\arabic{section}}

% Change date format to use it for version number
\usepackage[yyyymmdd]{datetime}
\renewcommand{\dateseparator}{.}


\title{Data Compression - Report}
\date{Version \today}
\author{lrfk99}

\begin{document}

\maketitle


\section{Dictionary}
Symbols are words, coded as 1 or 2 bytes, usually as a preprocessing step.
Use bytes rather than characters.
It would be possible to use only characters in the encoded file to represent either a number or letter or special character, 
but it would be much more efficient to use bytes to store information since these have the same size as a character (8 bits) 
but they can be much more useful and versatile for encoding a numerical value as well which would otherwise require 8 bits per 
digit.
Since standard ASCII has 256 possible characters and so requires 8 bits to store each character, 
we can reduce this number in our compression if we never see some of those characters, e.g. non-english characters which 
clearly will not be used in an english .tex document. As such we could use just 7 bits to encode each character if the document
only uses a maximum of 128 different characters. The number of bits used to encode each character should be stated at the 
start of the encoded file such that the decoder can correctly decode the file.

\section{Prediction by Partial Matching (PPM)}
Order-n e.g. o0,o1,o2,... - symbols are bytes, modeled by frequency distribution in context of last n bytes
order-n, modeled in longest context matched, but dropping to lower orders for byte counts of 0.
Since the .tex file will be typeset in English, it is possible to predict the next character, 
given the previous character(s). As such, a statistical model of english text is appropriate to use. 
This statistical model can be 'trained' on lots of english text such as Alice in Wonderland, 
and other .tex documents such as the lecture notes for this module. 
When using PPM, there are three methods for assigning frequencies to the escape symbol. 
The most appropriate method for this scenario is Method C since it takes into account the fact that 
some contexts can be followed by virtually any other character by giving the escape symbol 
an appropriate count, whilst not reducing the count of other symbols.

\section{Context mixing}
bits modeled by combining predictions of independent models
The cutting edge in lossless compression in terms of compression ratio is achieved by combining 
the next-symbol predictions of two or more statistical models, producing more accurate predictions 
than any model individually.

\section{Lempel-Ziv (LZ)}
Symbols are strings.
LZ77 - repeated strings are coded by offset and length of previous occurence
LZ Welch - repeats are coded as indexes into dynamically built dictionary
Reduced Offset LZ - LZW with multiple small dictionaries selected by context
LZ predictive - ROLZ with dictionary size of l

\section{Burrows-Wheeler Transform (BWT)}
Order-n e.g. o0,o1,o2,... - symbols are bytes, modeled by frequency distribution in context of last n bytes
Symbol Ranking - Order-n, modeled by time since last seen.
Burrows-Wheeler Transform - Bytes are sorted by context, then modeled by order-0 Symbol Ranking

\section{Dynamic Markov Coding}
Bits modeled by PPM

\section{Long Short Term Memory (LSTM)}
Long short term memory - Context Mixing using neural network models.



\cite{TextBenchmark}

\bibliographystyle{plain}
\bibliography{references}



\end{document}
