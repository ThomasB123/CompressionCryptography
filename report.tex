
\documentclass[a4paper, 11pt]{article}

\usepackage[margin=2cm]{geometry}

\usepackage{amsfonts, amsmath, amssymb, amsthm}
%\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[caption=false]{subfig}


\numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}	[equation]	{Theorem}
\newtheorem{conjecture}	[equation]	{Conjecture}
\newtheorem{corollary}	[equation]	{Corollary}
\newtheorem{definition}	[equation]	{Definition}
\newtheorem{example}	[equation]	{Example}
\newtheorem{lemma}		[equation]	{Lemma}
\newtheorem{problem}	[equation]	{Problem}
\newtheorem{proposition}[equation]	{Proposition}
\newtheorem{remark}		[equation]	{Remark}

\theoremstyle{definition}
\newtheorem{claim}		[equation]	{Claim}
\newtheorem{exercise}   {Exercise}  [section]
\newtheorem{answer}     {Answer}    [section]

\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

\newcommand{\GF}{\mathrm{GF}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\dH}{d_\mathrm{H}}
\newcommand{\dmin}{d_{\min}}
\newcommand{\wH}{w_\mathrm{H}}
\newcommand{\wmin}{w_{\min}}
\newcommand{\lcm}{\mathrm{lcm}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}

\renewcommand{\i}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\divides}{\,|\,}

\newcommand{\probability}{\mathbb{P}}
\newcommand{\expectation}{\mathbb{E}}


	\newcommand{\Important}[1]{\textcolor{red}{#1}}
	\newcommand{\Structure}[1]{\textcolor{blue}{#1}}
	\newcommand{\Reference}[1]{\textcolor{teal}{#1}}
	\newcommand{\Define}[1]{\textcolor{purple}{#1}}


\newcommand{\alphabet}[1]{\mathcal{#1}}

\newcommand{\code}[1]{\mathsf{#1}}
\newcommand{\Paritycheck}           {\code{PC}}
\newcommand{\Repetition}            {\code{R}}
\newcommand{\Hamming}               {\code{H}}
\newcommand{\Golay}                 {\code{G}}
\newcommand{\ReedMuller}            {\code{RM}}
\newcommand{\ReedSolomon}           {\code{RS}}
\newcommand{\GeneralizedReedSolomon}{\code{GRS}}
\newcommand{\Alternant}             {\code{A}}
\newcommand{\Goppa}                 {\code{\Gamma}}


\renewcommand{\thesection}{\arabic{section}}

% Change date format to use it for version number
\usepackage[yyyymmdd]{datetime}
\renewcommand{\dateseparator}{.}


\title{Data Compression - Report}
\date{Version \today}
\author{lrfk99}

\begin{document}

\maketitle


\section{Adaptive Huffman coding}
Adaptive Huffman coding, Huffman codes are dynamically recreated for better and faster compression.
Algorithm Vitter
Dictionary:
Symbols are words, coded as 1 or 2 bytes, usually as a preprocessing step.

\section{Context mixing}
See Further.
bits modeled by combining predictions of independent models
The cutting edge in lossless compression in terms of compression ratio is achieved by combining 
the next-symbol predictions of two or more statistical models, producing more accurate predictions 
than any model individually.
Prediction by Partial Matching (PPM)
Order-n e.g. o0,o1,o2,... - symbols are bytes, modeled by frequency distribution in context of last n bytes
order-n, modeled in longest context matched, but dropping to lower orders for byte counts of 0.
Since the .tex file will be typeset in English, it is possible to predict the next character, 
given the previous character(s). As such, a statistical model of english text is appropriate to use. 
This statistical model can be 'trained' on lots of english text such as Alice in Wonderland, 
and other .tex documents such as the lecture notes for this module. 
When using PPM, there are three methods for assigning frequencies to the escape symbol. 
The most appropriate method for this scenario is Method C since it takes into account the fact that 
some contexts can be followed by virtually any other character by giving the escape symbol 
an appropriate count, whilst not reducing the count of other symbols.
I considered using Dynamic Markov compression, which uses predictive arithmetic coding similar to prediction by partial matching (PPM), 
except that the input is predicted one bit at a time rather than one byte at a time
Bits modeled by PPM
\href{https://mattmahoney.net/dc/dce.html#Section_422}{Explanation}

\section{Lempel-Ziv-Welch}
Symbols are strings.
Lempel-Ziv-Welch (LZW)
LZ77 - repeated strings are coded by offset and length of previous occurence
LZ Welch - repeats are coded as indexes into dynamically built dictionary
Reduced Offset LZ - LZW with multiple small dictionaries selected by context
LZ predictive - ROLZ with dictionary size of l

\section{Burrows-Wheeler Transform (BWT)}
The Burrows-Wheeler Transform will rearrange a character string into runs of similar characters. 
This makes it very easy to use move-to-front transform and then run-length encoding.
Since in this scenario we are able to scan the entire file before encoding it, 
The BWT improves the efficiency of the compression algorithm without storing any extra data other than the 
position of the first original character. 
we can use the Burrows-Wheeler transform (BWT) converts a list of symbols into a much more structured list.
Order-n e.g. o0,o1,o2,... - symbols are bytes, modeled by frequency distribution in context of last n bytes
Symbol Ranking - Order-n, modeled by time since last seen.
Burrows-Wheeler Transform - Bytes are sorted by context, then modeled by order-0 Symbol Ranking
\href{https://en.wikipedia.org/wiki/Burrows-Wheeler_transform}{check this}

\section{Move-to-front transform}
The move-to-front (MTF) transform allows for encoding a stream of bytes, and performs particularly well on sorted byte streams.

\section{Run-length encoding}
Run-length encoding (RLE) is where the actual compression will happen, after the data has been transformed twice by 
BWT and MTF.

\href{https://sites.google.com/site/datacompressionguide/}{this is good}
\href{https://en.wikipedia.org/wiki/PAQ}{.}
\href{https://tarsa.github.io/lossless-benchmark/}{.}
\href{https://www.mattmahoney.net/dc/text.html}{Benchmark}


\cite{TextBenchmark}

\bibliographystyle{plain}
\bibliography{references}



\end{document}
